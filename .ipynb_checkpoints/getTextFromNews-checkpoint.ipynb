{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3d3756",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-13453d76af36>, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-13453d76af36>\"\u001b[1;36m, line \u001b[1;32m79\u001b[0m\n\u001b[1;33m    if !(text_data==None):\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "#한글 글자수 세기위해서 re.findall()\n",
    "\n",
    "def countHangul(text):\n",
    "    #텍스트로부터 한글을 읽는데, 단어를 개수로 셈(정규식사용)\n",
    "    hanCount=len(re.findall(u'[\\u3130-\\u318F\\uAC00-\\uD7A3]+',text))\n",
    "    return hanCount\n",
    "\n",
    "def getTextFromLink(url):\n",
    "    print(url)\n",
    "    try:\n",
    "        html=urllib.request.urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        err=e.read()\n",
    "        code=e.getcode()\n",
    "        return None\n",
    "    soupNews = BeautifulSoup(html,'html.parser')\n",
    "    #print(soupNews.prettify())\n",
    "    tag_tbody = soupNews.find('tbody')\n",
    "\n",
    "    #tag_br=soupNews.select_one(\"br\").parent.get_text().strip()\n",
    "\n",
    "    tag_p=soupNews.find_all(\"p\")\n",
    "    \n",
    "    #print(\"\\ntag_p\")\n",
    "    #print(tag_p)\n",
    "    #print(\"\\ntag_br\")\n",
    "    #print(tag_br)\n",
    "    return tag_p\n",
    "\n",
    "#p태그로 분리한 tag_p에서 일정개수 이상의 한글이 안나오면 본문이\n",
    "#아니라고 판단하면 될듯 ex 한글개수30개보다 적다. 이러면 해당 데이터는 날리고\n",
    "#이때 br태그로 다시 본문을 뽑아보고 그후에 다시한번 한글 확인후 날리면될드\n",
    "#뉴스사이트들마다 같은 기사가 있는것을 확인했음.\n",
    "#만약 이 두조건으로 검색안되는 뉴스본문은 날려도 다른 뉴스사이트 본문에서 같은\n",
    "#내용을 다뤄서 상관이 없을듯 싶음.\n",
    "\n",
    "\"\"\"\n",
    "def getTextFromBrTag(url):\n",
    "    print(url)\n",
    "    html=urllib.request.urlopen(url)\n",
    "    soupNews = BeautifulSoup(html,'html.parser')\n",
    "    #print(soupNews.prettify())\n",
    "    tag_tbody = soupNews.find('tbody')\n",
    "\n",
    "    tag_br=soupNews.select_one(\"br\").parent.get_text().strip()\n",
    "\n",
    "    #print(\"\\ntag_br\")\n",
    "    #print(tag_br)\n",
    "    return tag_br\n",
    "\"\"\"\n",
    "def getTextFromNewsJson(srcText):\n",
    "    with open(\"%s_naver_news.json\" %(srcText),'r',encoding=\"UTF-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "    link_data=[]\n",
    "    i=0\n",
    "    for data in json_data:\n",
    "        link_data.append(data['org_link'])\n",
    "        print(data)\n",
    "        i=i+1\n",
    "        if i==10:\n",
    "            break\n",
    "    #print('\\nlinkdata===\\n')\n",
    "    #print(link_data)\n",
    "    result=[]\n",
    "    \n",
    "    for link in link_data:\n",
    "        count_all=0\n",
    "        result_text=[]\n",
    "        final_text=''\n",
    "        text_data=getTextFromLink(link)\n",
    "        print(text_data)\n",
    "        if !(text_data==None):\n",
    "            for t in text_data:\n",
    "                count=countHangul(t.text)\n",
    "                if count>10:\n",
    "                    result_text.append(t.text)\n",
    "                    count_all=count_all+count\n",
    "        #result_text에 추가된 전체글을 다시 세본다.\n",
    "        #->만약 전체글이 50단어 미만이면 br태그로 재검색\n",
    "        #count = 0\n",
    "        #for t in result_text:\n",
    "            \n",
    "        #count=countHangul(text_data)\n",
    "        #print(count)\n",
    "        #print('\\n')\n",
    "        ##전체본문이50단어 미만이면 쓰레기 데이터라 판단\n",
    "            if count_all>=50:\n",
    "                final_text='\\n'.join(result_text)\n",
    "            result.append(final_text)\n",
    "        #전체 글중에서 result안의 한글을 전부 세서 50개미만이라던가\n",
    "        #특정 개수 이하면 br태그로 재검색하면 될듯.\n",
    "        #변경 -> p태그로 검색되는 뉴스들이 이미 거의 모든 br태그 뉴스를 포함함\n",
    "    resultTextToJson(srcText,result,json_data)\n",
    "   # print(\"==============++++++++++++++++++++++++++====재확인용\")\n",
    "    #for r in result:\n",
    "     #   print(r)\n",
    "      #  print('\\n')\n",
    "    return\n",
    "\n",
    "def resultTextToJson(srcText,result,newsJson):\n",
    "    jsonResult = []\n",
    "    i=0\n",
    "    for data in newsJson:\n",
    "        title=data['title']\n",
    "        description=data['description']\n",
    "        TEXT = result[i]\n",
    "        jsonResult.append({'title':title, 'description': description, 'TEXT':TEXT})\n",
    "        i=i+1\n",
    "        if i==10:\n",
    "            break\n",
    "    with open('%s_TEXT.json' % (srcText), 'w', encoding='utf8') as outfile:\n",
    "        jsonFile = json.dumps(jsonResult,  indent=4, sort_keys=True,  ensure_ascii=False)\n",
    "                        \n",
    "        outfile.write(jsonFile)\n",
    "    print(\"본문,요약,타이틀.json 완료\")\n",
    "    return\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b134bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
